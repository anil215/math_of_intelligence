{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network\n",
    "\n",
    "This is a pure numpy implementation of word generation using an RNN\n",
    "\n",
    "![alt text](http://corochann.com/wp-content/uploads/2017/05/text_sequence_predict.png \"Logo Title Text 1\")\n",
    "\n",
    "We're going to have our network learn how to predict the next words in a given paragraph. This will require a recurrent architecture since the network will have to remember a sequence of characters. The order matters. 1000 iterations and we'll have pronouncable english. The longer the training time the better. You can feed it any text sequence (words, python, HTML, etc.)\n",
    "\n",
    "## What is a Recurrent Network?\n",
    "\n",
    "Feedforward networks are great for learning a pattern between a set of inputs and outputs.\n",
    "![alt text](https://www.researchgate.net/profile/Sajad_Jafari3/publication/275334508/figure/fig1/AS:294618722783233@1447253985297/Fig-1-Schematic-of-the-multilayer-feed-forward-neural-network-proposed-to-model-the.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://s-media-cache-ak0.pinimg.com/236x/10/29/a9/1029a9a0534a768b4c4c2b5341bdd003--city-year-math-patterns.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.researchgate.net/profile/Hamza_Guellue/publication/223079746/figure/fig5/AS:305255788105731@1449790059371/Fig-5-Configuration-of-a-three-layered-feed-forward-neural-network.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "- temperature & location\n",
    "- height & weight\n",
    "- car speed and brand\n",
    "\n",
    "But what if the ordering of the data matters? \n",
    "\n",
    "![alt text](http://www.aboutcurrency.com/images/university/fxvideocourse/google_chart.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://news.mit.edu/sites/mit.edu.newsoffice/files/styles/news_article_image_top_slideshow/public/images/2016/vondrick-machine-learning-behavior-algorithm-mit-csail_0.jpg?itok=ruGmLJm2 \"Logo Title Text 1\")\n",
    "\n",
    "Alphabet, Lyrics of a song. These are stored using Conditional Memory. You can only access an element if you have access to the previous elements (like a linkedlist). \n",
    "\n",
    "Enter recurrent networks\n",
    "\n",
    "We feed the hidden state from the previous time step back into the the network at the next time step.\n",
    "\n",
    "![alt text](https://iamtrask.github.io/img/basic_recurrence_singleton.png \"Logo Title Text 1\")\n",
    "\n",
    "So instead of the data flow operation happening like this\n",
    "\n",
    "## input -> hidden -> output\n",
    "\n",
    "it happens like this\n",
    "\n",
    "## (input + prev_hidden) -> hidden -> output\n",
    "\n",
    "wait. Why not this?\n",
    "\n",
    "## (input + prev_input) -> hidden -> output\n",
    "\n",
    "Hidden recurrence learns what to remember whereas input recurrence is hard wired to just remember the immediately previous datapoint\n",
    "\n",
    "![alt text](https://image.slidesharecdn.com/ferret-rnn-151211092908/95/recurrent-neural-networks-part-1-theory-10-638.jpg?cb=1449826311 \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://www.mathworks.com/help/examples/nnet/win64/RefLayRecNetExample_01.png \"Logo Title Text 1\")\n",
    "\n",
    "RNN Formula\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*TUFnE2arCrMrCvxH.png \"Logo Title Text 1\")\n",
    "\n",
    "It basically says the current hidden state h(t) is a function f of the previous hidden state h(t-1) and the current input x(t). The theta are the parameters of the function f. The network typically learns to use h(t) as a kind of lossy summary of the task-relevant aspects of the past sequence of inputs up to t.\n",
    "\n",
    "Loss function\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1440/0*ZsEG2aWfgqtk9Qk5. \"Logo Title Text 1\")\n",
    "\n",
    "The total loss for a given sequence of x values paired with a sequence of y values would then be just the sum of the losses over all the time steps. For example, if L(t) is the negative log-likelihood\n",
    "of y (t) given x (1), . . . , x (t) , then sum them up you get the loss for the sequence \n",
    "\n",
    "\n",
    "## Our steps\n",
    "\n",
    "- Initialize weights randomly\n",
    "- Give the model a char pair (input char & target char. The target char is the char the network should guess, its the next char in our sequence)\n",
    "- Forward pass (We calculate the probability for every possible next char according to the state of the model, using the paramters)\n",
    "- Measure error (the distance between the previous probability and the target char)\n",
    "- We calculate gradients for each of our parameters to see their impact they have on the loss (backpropagation through time)\n",
    "- update all parameters in the direction via gradients that help to minimise the loss\n",
    "- Repeat! Until our loss is small AF\n",
    "\n",
    "## What are some use cases?\n",
    "\n",
    "- Time series prediction (weather forecasting, stock prices, traffic volume, etc. )\n",
    "- Sequential data generation (music, video, audio, etc.)\n",
    "\n",
    "## Other Examples\n",
    "\n",
    "-https://github.com/anujdutt9/RecurrentNeuralNetwork (binary addition)\n",
    "\n",
    "## What's next? \n",
    "\n",
    "1 LSTM Networks\n",
    "2 Bidirectional networks\n",
    "3 recursive networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The code contains 4 parts\n",
    "* Load the trainning data\n",
    "  * encode char into vectors\n",
    "* Define the Recurrent Network\n",
    "* Define a loss function\n",
    "  * Forward pass\n",
    "  * Loss\n",
    "  * Backward pass\n",
    "* Define a function to create sentences from the model\n",
    "* Train the network\n",
    "  * Feed the network\n",
    "  * Calculate gradient and update the model parameters\n",
    "  * Output a text to see the progress of the training\n",
    " \n",
    "\n",
    "## Load the training data\n",
    "\n",
    "The network need a big txt file as an input.\n",
    "\n",
    "The content of the file will be used to train the network.\n",
    "\n",
    "I use Methamorphosis from Kafka (Public Domain). Because Kafka was one weird dude. I like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 137628 chars, 80 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('kafka.txt', 'r').read()\n",
    "\n",
    "chars = list(set(data)) # contains all the characters the document is made up of\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('data has %d chars, %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode/Decode char/vector\n",
    "\n",
    "Neural networks operate on vectors (a vector is an array of float)\n",
    "So we need a way to encode and decode a char as a vector.\n",
    "\n",
    "We'll count the number of unique chars (*vocab_size*). That will be the size of the vector. \n",
    "The vector contains only zero exept for the position of the char wherae the value is 1.\n",
    "\n",
    "#### So First let's calculate the *vocab_size*:\n",
    "#### Then we create 2 dictionary to encode and decode a char to an int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'s': 0, 'E': 1, 'W': 2, '2': 5, '3': 11, 'r': 7, 'H': 8, 'y': 9, 'U': 13, 'a': 79, 'L': 78, 'e': 14, ';': 15, 'j': 17, '-': 18, '5': 20, 'Y': 19, '1': 6, 'n': 73, 'm': 31, 'K': 22, 'u': 77, 'o': 23, 'k': 24, 'S': 25, 'O': 26, ',': 27, 'M': 29, 'X': 30, '9': 32, '$': 39, 'G': 33, 'T': 34, '/': 38, 'c': 36, 'f': 37, 'F': 10, 'd': 35, 'N': 40, 'l': 21, 'A': 74, 'w': 41, 't': 42, 'z': 43, 'J': 44, \"'\": 45, ':': 46, '4': 47, '7': 48, '0': 49, 'g': 50, '\"': 51, 'R': 53, 'i': 54, 'h': 12, 'q': 56, ')': 60, 'v': 59, '6': 3, 'x': 61, 'I': 62, 'Q': 52, '?': 63, '8': 58, '\\n': 64, 'รง': 65, '*': 57, 'B': 67, '.': 68, 'C': 69, 'p': 70, 'D': 71, '%': 72, '!': 28, '(': 55, 'P': 75, ' ': 76, 'b': 66, '@': 4, 'V': 16}\n",
      "{0: 's', 1: 'E', 2: 'W', 3: '6', 4: '@', 5: '2', 6: '1', 7: 'r', 8: 'H', 9: 'y', 10: 'F', 11: '3', 12: 'h', 13: 'U', 14: 'e', 15: ';', 16: 'V', 17: 'j', 18: '-', 19: 'Y', 20: '5', 21: 'l', 22: 'K', 23: 'o', 24: 'k', 25: 'S', 26: 'O', 27: ',', 28: '!', 29: 'M', 30: 'X', 31: 'm', 32: '9', 33: 'G', 34: 'T', 35: 'd', 36: 'c', 37: 'f', 38: '/', 39: '$', 40: 'N', 41: 'w', 42: 't', 43: 'z', 44: 'J', 45: \"'\", 46: ':', 47: '4', 48: '7', 49: '0', 50: 'g', 51: '\"', 52: 'Q', 53: 'R', 54: 'i', 55: '(', 56: 'q', 57: '*', 58: '8', 59: 'v', 60: ')', 61: 'x', 62: 'I', 63: '?', 64: '\\n', 65: 'รง', 66: 'b', 67: 'B', 68: '.', 69: 'C', 70: 'p', 71: 'D', 72: '%', 73: 'n', 74: 'A', 75: 'P', 76: ' ', 77: 'u', 78: 'L', 79: 'a'}\n"
     ]
    }
   ],
   "source": [
    "char_to_ix = { ch:i for i,ch in enumerate(chars)}\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finaly we create a vector from a char like this:\n",
    "The dictionary defined above allows us to create a vector of size 80 instead of 256.  \n",
    "Here and exemple of the char 'a'  \n",
    "The vector contains only zeros, except at position char_to_ix['a'] where we put a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of the network\n",
    "\n",
    "The neural network is made of 3 layers:\n",
    "* an input layer\n",
    "* an hidden layer\n",
    "* an output layer\n",
    "\n",
    "All layers are fully connected to the next one: each node of a layer are conected to all nodes of the next layer.\n",
    "The hidden layer is connected to the output and to itself: the values from an iteration are used for the next one.\n",
    "\n",
    "To centralise values that matter for the training (_hyper parameters_) we also define the _sequence lenght_ and the _learning rate_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#hyperparameters\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model parameters\n",
    "\n",
    "Wxh = np.random.randn(hidden_size, vocab_size) * 0.01 #input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size) * 0.01 #input to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size) * 0.01 #input to hidden\n",
    "bh = np.zeros((hidden_size, 1))\n",
    "by = np.zeros((vocab_size, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model parameters are adjusted during the trainning.\n",
    "* _Wxh_ are parameters to connect a vector that contain one input to the hidden layer.\n",
    "* _Whh_ are parameters to connect the hidden layer to itself. This is the Key of the Rnn: Recursion is done by injecting the previous values from the output of the hidden state, to itself at the next iteration.\n",
    "* _Why_ are parameters to connect the hidden layer to the output\n",
    "* _bh_ contains the hidden bias\n",
    "* _by_ contains the output bias\n",
    "\n",
    "You'll see in the next section how theses parameters are used to create a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss function\n",
    "\n",
    "The __loss__ is a key concept in all neural networks training. \n",
    "It is a value that describe how good is our model.  \n",
    "The smaller the loss, the better our model is.  \n",
    "(A good model is a model where the predicted output is close to the training output)\n",
    "  \n",
    "During the training phase we want to minimize the loss.\n",
    "\n",
    "The loss function calculates the loss but also the gradients (see backward pass):\n",
    "* It perform a forward pass: calculate the next char given a char from the training set.\n",
    "* It calculate the loss by comparing the predicted char to the target char. (The target char is the input following char in the tranning set)\n",
    "* It calculate the backward pass to calculate the gradients \n",
    "\n",
    "This function take as input:\n",
    "* a list of input char\n",
    "* a list of target char\n",
    "* and the previous hidden state\n",
    "\n",
    "This function outputs:\n",
    "* the loss\n",
    "* the gradient for each parameters between layers\n",
    "* the last hidden state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward pass\n",
    "The forward pass use the parameters of the model (Wxh, Whh, Why, bh, by) to calculate the next char given a char from the trainning set.\n",
    "\n",
    "xs[t] is the vector that encode the char at position t\n",
    "ps[t] is the probabilities for next char\n",
    "\n",
    "![alt text](https://deeplearning4j.org/img/recurrent_equation.png \"Logo Title Text 1\")\n",
    "\n",
    "```python\n",
    "hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "```\n",
    "\n",
    "or is dirty pseudo code for each char\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh\n",
    "ys = hs*Why + by\n",
    "ps = normalized(ys)\n",
    "```\n",
    "\n",
    "### Backward pass\n",
    "\n",
    "The naive way to calculate all gradients would be to recalculate a loss for small variations for each parameters.\n",
    "This is possible but would be time consuming.\n",
    "There is a technics to calculates all the gradients for all the parameters at once: the backdrop propagation.  \n",
    "Gradients are calculated in the oposite order of the forward pass, using simple technics.  \n",
    "\n",
    "#### goal is to calculate gradients for the forward formula:\n",
    "```python\n",
    "hs = input*Wxh + last_value_of_hidden_state*Whh + bh  \n",
    "ys = hs*Why + by\n",
    "```\n",
    "\n",
    "The loss for one datapoint\n",
    "![alt text](http://i.imgur.com/LlIMvek.png \"Logo Title Text 1\")\n",
    "\n",
    "How should the computed scores inside f change tto decrease the loss? We'll need to derive a gradient to figure that out.\n",
    "\n",
    "Since all output units contribute to the error of each hidden unit we sum up all the gradients calculated at each time step in the sequence and use it to update the parameters. So our parameter gradients becomes :\n",
    "\n",
    "![alt text](http://i.imgur.com/Ig9WGqP.png \"Logo Title Text 1\")\n",
    "\n",
    "Our first gradient of our loss. We'll backpropagate this via chain rule\n",
    "\n",
    "![alt text](http://i.imgur.com/SOJcNLg.png \"Logo Title Text 1\")\n",
    "\n",
    "The chain rule is a method for finding the derivative of composite functions, or functions that are made by combining one or more functions.\n",
    "\n",
    "![alt text](http://i.imgur.com/3Z2Rfdi.png \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](http://mathpullzone-8231.kxcdn.com/wp-content/uploads/thechainrule-image3.jpg \"Logo Title Text 1\")\n",
    "\n",
    "![alt text](https://i0.wp.com/www.mathbootcamps.com/wp-content/uploads/thechainrule-image1.jpg?w=900 \"Logo Title Text 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def lossFun(inputs, targets, hprev):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  inputs,targets are both list of integers.                                                                                                                                                   \n",
    "  hprev is Hx1 array of initial hidden state                                                                                                                                                  \n",
    "  returns the loss, gradients on model parameters, and last hidden state                                                                                                                      \n",
    "  \"\"\"\n",
    "  #store our inputs, hidden states, outputs, and probability values\n",
    "    # Each of these are going to be SEQ_LENGTH(Here 25) long dicts i.e. 1 vector per time(seq) step\n",
    "    # xs will store 1 hot encoded input characters for each of 25 time steps (26, 25 times)\n",
    "    # hs will store hidden state outputs for 25 time steps (100, 25 times)) plus a -1 indexed initial state\n",
    "    # to calculate the hidden state at t = 0\n",
    "    # ys will store targets i.e. expected outputs for 25 times (26, 25 times), unnormalized probabs\n",
    "    # ps will take the ys and convert them to normalized probab for chars\n",
    "    # We could have used lists BUT we need an entry with -1 to calc the 0th hidden layer\n",
    "    # -1 as  a list index would wrap around to the final element\n",
    "  xs, hs, ys, ps = {}, {}, {}, {}\n",
    "  #init with previous hidden state\n",
    "    # Using \"=\" would create a reference, this creates a whole separate copy\n",
    "    # We don't want hs[-1] to automatically change if hprev is changed\n",
    "  hs[-1] = np.copy(hprev)\n",
    "  #init loss as 0\n",
    "  loss = 0\n",
    "  # forward pass                                                                                                                                                                              \n",
    "  for t in range(len(inputs)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation (we place a 0 vector as the t-th input)                                                                                                                     \n",
    "    xs[t][inputs[t]] = 1 # Inside that t-th input we use the integer in \"inputs\" list to  set the correct\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state                                                                                                            \n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars                                                                                                           \n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars                                                                                                              \n",
    "    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)                                                                                                                       \n",
    "  # backward pass: compute gradients going backwards    \n",
    "  #initalize vectors for gradient values for each set of weights \n",
    "  dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "  dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "  dhnext = np.zeros_like(hs[0])\n",
    "  for t in reversed(range(len(inputs))):\n",
    "    #output probabilities\n",
    "    dy = np.copy(ps[t])\n",
    "    #derive our first gradient\n",
    "    dy[targets[t]] -= 1 # backprop into y  \n",
    "    #compute output gradient -  output times hidden states transpose\n",
    "    #When we apply the transpose weight matrix,  \n",
    "    #we can think intuitively of this as moving the error backward\n",
    "    #through the network, giving us some sort of measure of the error \n",
    "    #at the output of the lth layer. \n",
    "    #output gradient\n",
    "    dWhy += np.dot(dy, hs[t].T)\n",
    "    #derivative of output bias\n",
    "    dby += dy\n",
    "    #backpropagate!\n",
    "    dh = np.dot(Why.T, dy) + dhnext # backprop into h                                                                                                                                         \n",
    "    dhraw = (1 - hs[t] * hs[t]) * dh # backprop through tanh nonlinearity                                                                                                                     \n",
    "    dbh += dhraw #derivative of hidden bias\n",
    "    dWxh += np.dot(dhraw, xs[t].T) #derivative of input to hidden layer weight\n",
    "    dWhh += np.dot(dhraw, hs[t-1].T) #derivative of hidden layer to hidden layer weight\n",
    "    dhnext = np.dot(Whh.T, dhraw) \n",
    "  for dparam in [dWxh, dWhh, dWhy, dbh, dby]:\n",
    "    np.clip(dparam, -5, 5, out=dparam) # clip to mitigate exploding gradients                                                                                                                 \n",
    "  return loss, dWxh, dWhh, dWhy, dbh, dby, hs[len(inputs)-1]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a sentence from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " U14iy'0?aEwO\n",
      "umw6VRLQ-kc/hiWaVwLJp*ahD0UA(3Gig9zVW  gePF5e*xwSFHDBv\"O)b?;pgSsMHVB!ob:1xISCWr!9RKrX:LHw T8WE\n",
      "N5\n",
      "B)** h92U-GEJs-9w69L,Jpn@CXkADc3I@JfqPpUhXOwO4e%q5)XYLHh*PUM'D;IBD\"62@Scu\n",
      "G6HOytSPLgรงA/Te \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "#prediction, one full forward pass\n",
    "def sample(h, seed_ix, n):\n",
    "  \"\"\"                                                                                                                                                                                         \n",
    "  sample a sequence of integers from the model                                                                                                                                                \n",
    "  h is memory state, seed_ix is seed letter for first time step   \n",
    "  n is how many characters to predict\n",
    "  \"\"\"\n",
    "  #create vector\n",
    "  x = np.zeros((vocab_size, 1))\n",
    "  #customize it for our seed char\n",
    "  x[seed_ix] = 1\n",
    "  #list to store generated chars\n",
    "  ixes = []\n",
    "  #for as many characters as we want to generate\n",
    "  for t in range(n):\n",
    "    #a hidden state at a given time step is a function \n",
    "    #of the input at the same time step modified by a weight matrix \n",
    "    #added to the hidden state of the previous time step \n",
    "    #multiplied by its own hidden state to hidden state matrix.\n",
    "    h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "    #compute output (unnormalised)\n",
    "    y = np.dot(Why, h) + by\n",
    "    ## probabilities for next chars\n",
    "    p = np.exp(y) / np.sum(np.exp(y))\n",
    "    #pick one with the highest probability \n",
    "    ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "    #create a vector\n",
    "    x = np.zeros((vocab_size, 1))\n",
    "    #customize it for the predicted char\n",
    "    x[ix] = 1\n",
    "    #add it to the list\n",
    "    ixes.append(ix)\n",
    "\n",
    "  txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "  print('----\\n %s \\n----' % (txt, ))\n",
    "hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Training\n",
    "\n",
    "This last part of the code is the main trainning loop:\n",
    "* Feed the network with portion of the file. Size of chunk is *seq_lengh*\n",
    "* Use the loss function to:\n",
    "  * Do forward pass to calculate all parameters for the model for a given input/output pairs\n",
    "  * Do backward pass to calculate all gradiens\n",
    "* Print a sentence from a random seed using the parameters of the network\n",
    "* Update the model using the Adaptative Gradien technique Adagrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Feed the loss function with inputs and targets\n",
    "\n",
    "We create two array of char from the data file,\n",
    "the targets one is shifted compare to the inputs one.\n",
    "\n",
    "For each char in the input array, the target array give the char that follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs [26, 73, 14, 76, 31, 23, 7, 73, 54, 73, 50, 27, 76, 41, 12, 14, 73, 76, 33, 7, 14, 50, 23, 7, 76]\n",
      "targets [73, 14, 76, 31, 23, 7, 73, 54, 73, 50, 27, 76, 41, 12, 14, 73, 76, 33, 7, 14, 50, 23, 7, 76, 25]\n"
     ]
    }
   ],
   "source": [
    "p=0  \n",
    "inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "print(\"inputs\", inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "print(\"targets\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adagrad to update the parameters\n",
    "\n",
    "This is a type of gradient descent strategy\n",
    "\n",
    "![alt text](http://www.logos.t.u-tokyo.ac.jp/~hassy/deep_learning/adagrad/adagrad2.png\n",
    " \"Logo Title Text 1\")\n",
    "\n",
    "\n",
    "\n",
    "step size = learning rate\n",
    "\n",
    "The easiest technics to update the parmeters of the model is this:\n",
    "\n",
    "```python\n",
    "param += dparam * step_size\n",
    "```\n",
    "Adagrad is a more efficient technique where the step_size are getting smaller during the training.\n",
    "\n",
    "It use a memory variable that grow over time:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "```\n",
    "and use it to calculate the step_size:\n",
    "```python\n",
    "step_size = 1./np.sqrt(mem + 1e-8)\n",
    "```\n",
    "In short:\n",
    "```python\n",
    "mem += dparam * dparam\n",
    "param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update \n",
    "```\n",
    "\n",
    "### Smooth_loss\n",
    "\n",
    "Smooth_loss doesn't play any role in the training.\n",
    "It is just a low pass filtered version of the loss:\n",
    "```python\n",
    "smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "```\n",
    "\n",
    "It is a way to average the loss on over the last iterations to better track the progress\n",
    "\n",
    "\n",
    "### So finally\n",
    "Here the code of the main loop that does both trainning and generating text from times to times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 109.550671\n",
      "----\n",
      " cftN4KrE,KMx)b7)5B\n",
      "obMJWRL'!e$'L2l4M\n",
      "a3@u yKd%-va/)VvMrRI33N,Pqu%(XVhroe)nwpq\n",
      "v1xqua6yMOOm9uaq'/3X,uetru/L-RK-1S@jp6LX'AGMoO*qQzbh Kr2\n",
      "i,IeEรงOv1L-iHJ(yLe?MwYw S:7M*GDM4eSQe\n",
      " QOB5vv7ocs1-รง'F3DKPรง2wHV$O \n",
      "----\n",
      "iter 1000, loss: 86.463275\n",
      "----\n",
      "  isurres meveris th on tre, ohw,nf oatwthems gd mis se tpe yvanegirev acfe shecantav'dg Gdhr wohet be yae abokflt, wostwaad ulnm cat d ud iperdoacb'nerur woaran coctrerer rte wol, r cooms m oh re the  \n",
      "----\n",
      "iter 2000, loss: 70.069266\n",
      "----\n",
      " lt hedonthasher ume dor acob. he tasif lived thasghadhis hathae thad tong. Iobeg asr av he one, panin tomelilgy pockon the w h, hrouneiwhincherss s-wemsy, torl tholyorther tounghad hes he toy ched and \n",
      "----\n",
      "iter 3000, loss: 61.838632\n",
      "----\n",
      "  was he whe tad cienlnt ansdutse bl b,ml jsot hathacung ne lar thas pes muwurgeget, wwad wohaye tuly ele. rsed hat eud yr inylentor comim ar;er mid bimt; te sot sorclang coascind Gringy Hey sot inlld  \n",
      "----\n",
      "iter 4000, loss: 58.000873\n",
      "----\n",
      " , bove th ackey the sount to they fomir hithin. Ther, Grecolged ant the, ris sttome moreghead go ramoted wiag woromn inam be stsor. He h or geap cong. Nousty math rothany bthis fith branltend gtasgy f \n",
      "----\n",
      "iter 5000, loss: 59.698662\n",
      "----\n",
      " inmecenthosect tobes ils co counund thersd theratit wernrthes yiar, PPedelstem inantd cokis I.\n",
      "Mmerpe\n",
      "re \"inig woDmeghaptirlericmind Tissir that Elit ther wad che to thay as: envoprenter therriilg tha \n",
      "----\n",
      "iter 6000, loss: 60.435447\n",
      "----\n",
      " dobonge he ther the gfok de faveng road hiemt wHo waving eid aviple ro of her whubed seallithin of lbet ro soud the fopirn als, ivere merstute festnce\n",
      " wham pewhe Ilis  a whald varh lete mexs Geel whe \n",
      "----\n",
      "iter 7000, loss: 56.077793\n",
      "----\n",
      "  furd, hath\n",
      "\n",
      "\"wevet ave\"d ffa aff hid inmi he weilu foust of as thepliglextt oofr pris lo, abekry om, asas wis urpate rasablebe the incentcke aple sous los and he flatat), Grer the fosegh, droms hid t \n",
      "----\n",
      "iter 8000, loss: 52.825170\n",
      "----\n",
      " te to sowl o beow boseben had prod pis agaac. Wast onsle ene en ibcem, and af wned thelve wof the qlon tud meleen the nane thas he cinith hille beared has and he neielm, th here ris mwargo coogher cos \n",
      "----\n",
      "iter 9000, loss: 51.116861\n",
      "----\n",
      " eayser ang thede copper anlytle that most thald ater!, for ther as aningoyer, intad artande his of bapden forowesly isly waut int has daste the momok, alingeres geand hew gerontent the ening qut of co \n",
      "----\n",
      "iter 10000, loss: 50.488621\n",
      "----\n",
      "  therrte thee bick trpeor. Wow inted thel'g bithe souch inle holly on tionies fold, of tultunate yild the or hholfed swat was aifiut drok, en. Mre, saiiming, coor not kiove whas sace woop to the reen  \n",
      "----\n",
      "iter 11000, loss: 57.195852\n",
      "----\n",
      " n - sayboj Pracaary cannt, Shisemen wast\n",
      "5Rermfauneverat withe. timect Projetl\n",
      "Miceut taten. ound ins wookent ition thor with -edibele theid perment Lor wcougate Foug\n",
      "necem Me rmeom oxtey.\n",
      " - difape\n",
      "c \n",
      "----\n",
      "iter 12000, loss: 53.794246\n",
      "----\n",
      " wasent egrtere the thist, ina. Sa hing thet thy vo liment not aver ss; he to sawat his ant ofr corRnimln allasn he fall tumeld as ines woritiw, fur appe stenk. sherp's whaped sen turt in agcaning fory \n",
      "----\n",
      "iter 13000, loss: 50.482702\n",
      "----\n",
      " oughaningiag at hle cont naid hree at on alf-reve\n",
      "he entherowadiin luthef Grest thr beay Grogben for the lotongen st rately. The meanctockius, dougis movevery thingibe.\n",
      "I cont ond paconst of gh iterev \n",
      "----\n",
      "iter 14000, loss: 48.590632\n",
      "----\n",
      " inghine fas the soor onspcofod yathery to him thindy, bulk of lleriwhinns erlavied and dow thicked writone an hin awpububas sooj, iis had suicheved stome were shat fast his sowerdthon foved Gred whe w \n",
      "----\n",
      "iter 15000, loss: 47.897811\n",
      "----\n",
      " haroutide un't - ameld moth had of gott, deshed. re wook.\n",
      "\"would surpates iot\n",
      "\n",
      "Weeshand was outhere nelh - mull so room himst\" wess. his pather thas d chatiig heed stak fere intanin le aplepened lnas  \n",
      "----\n",
      "iter 16000, loss: 50.860307\n",
      "----\n",
      " r\n",
      "ounhtork the in whes worifrighs Gregt tameny intars oord whe conerent; thondl\n",
      "herect uwarr Ssy not fut of thiy in yoouther an whior ched a.\n",
      "\n",
      "Whasese chaecsigecing iteld aut Prting to with and ceacke \n",
      "----\n",
      "iter 17000, loss: 52.824024\n",
      "----\n",
      " y hous sping.\n",
      "Prighad ble peing his hes ither ans hapy and queloro worpe wand for, bor ondidsen lyttint\" follomat ss monhige hes face on of and the ghroomin the to the wcom conion store vom not hay fo \n",
      "----\n",
      "iter 18000, loss: 49.887449\n",
      "----\n",
      " lly he witfiblay lWowh sow sted at exwither at wish olfalluld on strimsatten sseas; frough had it to wars want at arde thout ef whellar siss way nall s with he the us treicat but hit in unser bestryan \n",
      "----\n",
      "iter 19000, loss: 47.592273\n",
      "----\n",
      " solis ats sperent uf to h. He\". \"He ward at morg; Ble; and as dss ap she daves, lil quidenf with und they neatiry he not mors of hise th erventlis ibririth he hit the would exthto shes ved habr tis si \n",
      "----\n",
      "iter 20000, loss: 46.749564\n",
      "----\n",
      " ung he cawlure wollained thand her enante ay m she foust ands they ard tood the leams the his alt fragh sposer thear piling lead way and haveny samednasing haw an not that beciir tor jun) dofurar exob \n",
      "----\n",
      "iter 21000, loss: 46.636043\n",
      "----\n",
      " kednp thoutior ems ho gouls, would it his father it then she to root his shatland lead ewasted flut when hagr everfued so Unon ttingpalidnon. He, slayt beed thiint dark. Ared and tor syouk und brco th \n",
      "----\n",
      "iter 22000, loss: 52.593151\n",
      "----\n",
      " ect amis, ofe orfire Mbecanerbebect worll gooms puteay aflove\n",
      "loder\n",
      "the turcceinss that \n",
      "fanns wcoucctione\" Gutect ontnem ca cergor's\n",
      " oulm\n",
      "jughtion thoryidst \"\n",
      "Gutcind woule they hics, conal8 for flu \n",
      "----\n",
      "iter 23000, loss: 50.127922\n",
      "----\n",
      " en's. \"O ceither, Inonightr woulregr ally the he cabbting thontt. He his lisped frewarrssem and Ickik, wher had betered. Hibuld sime list that ther couteid and surberegrtanad wint; to cryold on him th \n",
      "----\n",
      "iter 24000, loss: 47.443988\n",
      "----\n",
      " 'm awad the chem cout, wous if sheos wave whou, stet he yculd cielor as hou, As had nooment toun aflided as, he he d fer had  in the off ure get rom nout frith say't woul suthally therven heurnorg to  \n",
      "----\n",
      "iter 25000, loss: 45.849283\n",
      "----\n",
      " s had he works, to her said of he alad lesey sid the there siswoa't on have to hes in la hey canaay to hame into wauct have aras. Thined an\", yon wasce batann not the tot it would juthor's froj treas  \n",
      "----\n",
      "iter 26000, loss: 45.561974\n",
      "----\n",
      " ncaplpien iagh thouk dade eglthevendiate fate the dullow ever. Whe wenings womy the unst head so the hars, geste ut tooke peal, thine Gregorce the kloit, willy when and of horped worevest himlomchied  \n",
      "----\n",
      "iter 27000, loss: 48.200089\n",
      "----\n",
      " onol\n",
      ":\n",
      "\"Yo was atias in wosatet inbargedis, thating out in atheiegace the somely lo, awly theirpenielss ander wathing as acled cone the resele pereping; Pragwit, to becely frojat y Gungobate Vining wi \n",
      "----\n",
      "iter 28000, loss: 50.263713\n",
      "----\n",
      "  At at itRor or with in anding so  gond nisted saw in le hadle, nomay pite he onow alf of op lasthor wou;lsfn thence!F. ve fare atstle look in she sa wersle atle the me mall wI westing, just dutis, be \n",
      "----\n",
      "iter 29000, loss: 47.768593\n",
      "----\n",
      " Gregolf had the ckealnowlring his speark had nown't tat thancard amarile sad dickngor'g bion haw jf. Ther sight and of clearuser dithen by the thit what lark hils fout at she that neess with, lid con  \n",
      "----\n",
      "iter 30000, loss: 45.671978\n",
      "----\n",
      " icr acler. Ho the ferengimitg Ananet kithied had soughon. AGrcpomed wourd an hape aped preat buigg domy agate could to would amsed pledly the gater autic! Shich crefiring his erengo he his yout do me! \n",
      "----\n",
      "iter 31000, loss: 45.097158\n",
      "----\n",
      " d be ho tha hand fanted waran ither, him, the sacay. \"Gregored dive ver seot store. He what 4 the ble, what way, and the had sibe it ponethaght preatene hery an fatheronetoud whene ser to his suse onf \n",
      "----\n",
      "iter 32000, loss: 45.039892\n",
      "----\n",
      " e to bviredrood hadl als of and not out to donge whing wist, wir erstents nool boht n ind not he coplow surfed ast, neakle thabreit and listurn lett Gregonther thad out wher ad ardent a lole sang; soi \n",
      "----\n",
      "iter 33000, loss: 50.486696\n",
      "----\n",
      "  dlose, as in do terming, just atoid fr PrPthis on if incerte boseculd wenectabugmains, thow to we love's and anxirandowed\n",
      "ENNighy tracus out to mopens meates, of concisG\n",
      "parinewn, Gregisong fot Preit \n",
      "----\n",
      "iter 34000, loss: 48.437694\n",
      "----\n",
      " y him iteferdzes epanectut do mommerss enen thath rake to iode, at of he becent as encas firanly the right wist peanst., I hibler'sing he kich ttlle had of did thiteced fcoust thenponting him befle Li \n",
      "----\n",
      "iter 35000, loss: 45.900123\n",
      "----\n",
      " ted ther him to cout all in thand wolfeclecth, quid srocesh lead lead Grcewas was fickion y.s and withoutly as was hay gathered ait nood ando esteld andor the suping he touse tuld the legreden seasion \n",
      "----\n",
      "iter 36000, loss: 44.433946\n",
      "----\n",
      "  goor Gregond bets to shint. am abroig his bythis agrent hid it jonh alarly, GregoGbect, thetliuld no the cher hose sloubd asched saus he white wiat che he thel out as beste wablle it to the lomen sto \n",
      "----\n",
      "iter 37000, loss: 44.317020\n",
      "----\n",
      " steod ont oul he witint the feredncle himat! stmest anverinst lled as pock anf oned no klive unwipe ain was form of cood now theed corfer, they antor futedien. Hit out lech his fall fit hind on, would \n",
      "----\n",
      "iter 38000, loss: 46.666908\n",
      "----\n",
      " er't of was al\n",
      "ore room thiw crimen\"ed would \" tis it ingonichiincoiviinstiok tistergott fob dopainen they out and or of win out it workmsed; Guthenct wackey agrifally prome no his if iting his laby p \n",
      "----\n",
      "iter 39000, loss: 48.834295\n",
      "----\n",
      " bepse craite to nod bacigh ul, work the prouban that ste go of condonnzning aid certsels pweas be in there.\n",
      "\n",
      "Antiem shet oms biveed, and on ipmaus Youdcakes exprojelid handy theay careine; of antletio \n",
      "----\n",
      "iter 40000, loss: 46.542016\n",
      "----\n",
      " etming he ceat ffectsinctm him for had or his dyen.9y erpale primirge he gate offuned Gregor hand cal; and they to ceapen not!\" dnen, that haflion to goor for to the fors and restr wom hely dowlen she \n",
      "----\n",
      "iter 41000, loss: 44.538238\n",
      "----\n",
      " veave iting to castron has had ditholy projen, had ald ovit worrarsiand hurble. Souct has fied hering he whay, om sistere his stporgenen essisict bund the cherpetick the shen sIougras wored ke toon ma \n",
      "----\n",
      "iter 42000, loss: 44.090221\n",
      "----\n",
      " y him see best sear to the prongrees shemit9on yat or being the haplar heartion sead Grcjung mould smoofftling his facheld the overesherd lim ho wheor the courd exsole aw arbinno frobbe or on had teat \n",
      "----\n",
      "iter 43000, loss: 44.077885\n",
      "----\n",
      " mmo his not pris. Gregor's bled s upachent Gregerat'l hat whard to kide thand in to Guched.\n",
      "They inlyiseby seem of not watl, lookes. bl\". Bpane forer.\". He not. His mall op and to bequrs muth.\"'s Grin \n",
      "----\n",
      "iter 44000, loss: 49.040308\n",
      "----\n",
      " he dondyut,  tle. CF Unpen:cr ors thanenat tise arve -teas liins agrect Gut notw H. FI's work staraomy condore Lisaibiriot ittous or Gregoncm, tray his if caiter and beftas, for foors\n",
      "privergtrent som \n",
      "----\n",
      "iter 45000, loss: 47.363689\n",
      "----\n",
      " fed pimest ibed on had now to to imar, thasing had not stulledikg her a lind lampy any forbing loor nomple of \"/ough exton not lemply, of had feen at's foust ewadd. The seere  Projen motkey, bus ecang \n",
      "----\n",
      "iter 46000, loss: 44.941908\n",
      "----\n",
      " thing nnojed ereay,  ale and they from in oformering to here to be his aictle quente seass parne crood and was ever a listly becelate would from sead he whe's he had onareed bace enfither and abound y \n",
      "----\n",
      "iter 47000, loss: 43.545009\n",
      "----\n",
      " to caping hearsss om cand to the chected thet Gregor to ond cercone and to seitar us to ghey froms he geplin brenton; bus to to courshed eveaws to onod dow hive frasicceed ey it that, terenth had hit  \n",
      "----\n",
      "iter 48000, loss: 43.466118\n",
      "----\n",
      " m, ensed had plearts sowed wantarinther the craburverttredrnnerole the rooms shis claurf and forentir lack he iranent at hime who fof in the that as ever, wable; olla cask apabechnousied forsele\" form \n",
      "----\n",
      "iter 49000, loss: 45.649893\n",
      "----\n",
      " Preander lonst and at catentane outer the where goter\n",
      "201 Nnowh neicade farded works satim) she ening. - gone and kuemen serowinse it, frothoug frockny, up Grtenbicest in moams ruteared andey,\n",
      "\n",
      "Ice cc \n",
      "----\n",
      "iter 50000, loss: 47.854542\n",
      "----\n",
      " then lall, If we   got, of uflr stmp then to Unating, bed as a Prpop gether exlibbore get it and lvention straigl itwouser and of his sanntt's intrems at he dasest to mle pud sofulf ast recher it bros \n",
      "----\n",
      "iter 51000, loss: 45.708395\n",
      "----\n",
      " d olfust of a moull, to haw ale the cand not ungood sis remed noater ho lus atten with plaisen's on hit jectunt hidnef had the samrontaked. I'w woulf right. you and ontiller preast theed und empelf li \n",
      "----\n",
      "iter 52000, loss: 43.787464\n",
      "----\n",
      " dly thiediof id without the yat on alrew thinge his andly int on ingor's she me thark. Hers  quitrout wound whosherd vousfed, do sastect him! to cabedure anis noveg. But was some the sward terelr the  \n",
      "----\n",
      "iter 53000, loss: 43.429545\n",
      "----\n",
      " lmintessily enko. Gregor to resly the hasted nearmiat and. The poribuct. geflabll the bally fast him.) Gregov late chang as the mojitill his expreight no ther thelt lain miresamion of the sakg parall  \n",
      "----\n",
      "iter 54000, loss: 43.370954\n",
      "----\n",
      " od have had father at some. Whly Grostels swind; if,er cound aganded hildem the the fiting if becally. I tis his age whensist was loed coeve the whey afled to bowngor. Whering shead alping and way now \n",
      "----\n",
      "iter 55000, loss: 47.965726\n",
      "----\n",
      " ane ust quptinttely\n",
      "(ot lar pure. S. Filptiremained paice pistpovicedatidion frop that daclinntabughture here that it it to Yot morer\n",
      "porbutiout we corgher.  Uilfaines aratredtented withe for thnenita \n",
      "----\n",
      "iter 56000, loss: 46.573908\n",
      "----\n",
      " iby Gregor siedusted, and sough fient, dere chen ghere as and the yought he couthey it wive od - could wcarkat the thatherk, ges oplinst. Sorquling thacly a thad of a itry on the would twe koum seve f \n",
      "----\n",
      "iter 57000, loss: 44.249448\n",
      "----\n",
      " paction had but of one they thing his fould tinto mavezet to lity? Theed in all, didleff? s: af he had quim befontly not watlat; any lims weTh in Greatis doom he worw? Gregor his prast thise hidfe tha \n",
      "----\n",
      "iter 58000, loss: 42.939597\n",
      "----\n",
      " ing he frent he wnereevery not on trond the mo tat becent hid coss, vear aimust ancough; Greasmen to lile thit grint falmer, od, usted to expaly light aims on the live tor tooride nod tuteden what, be \n",
      "----\n",
      "iter 59000, loss: 42.850786\n",
      "----\n",
      " ethers ovoun bewteind shoughs hister. Sop meed enen't. Grepting hteed cotson to thestreard, tan. in slould loon wixted then of offt her get. Had broiflly dit dive atce the for, on and ap everyon him t \n",
      "----\n",
      "iter 60000, loss: 44.958201\n",
      "----\n",
      " m terso thres-s whith let vere slifingsuinser theik shon out o morepst eme but his alled the cera: pater in moreay got of the gow in with tars, h. Gregong d/on thioray them her woulchinglacm, On wad G \n",
      "----\n",
      "iter 61000, loss: 47.117829\n",
      "----\n",
      " , had ig but ous tous ragertains ancore contrene yoking whice room of wish have would ho coms he  the fitheut - bemer thoun wherd to brom agentention, all bught of it himalide's erost uned he had bet  \n",
      "----\n",
      "iter 62000, loss: 45.096730\n",
      "----\n",
      " ow theed lalsstly restal tan sisteftungo whing sost, Prophed puthe paiming of the saited with aly room, hastelf. Shan fo there at of this not the verircthter the otherer nowrs eictatlr it carmicate cl \n",
      "----\n",
      "iter 63000, loss: 43.211815\n",
      "----\n",
      " or. The eese newing, beacing, in the to the dill frencuarenly could vide aceffiling wonsarded it Fa have of thoug-tem and the without would an his miced ad dif hat fout out his his as store the to wai \n",
      "----\n",
      "iter 64000, loss: 42.869968\n",
      "----\n",
      " ritedres seast the for thop quit, of could had of all of would som homoin nojerins \"latne he warxpiotill; of thet he benicull ever fing ragered.\n",
      "\n",
      "He  Ither fure, tele and upean whoped effef himdole th \n",
      "----\n",
      "iter 65000, loss: 42.827498\n",
      "----\n",
      " 1 at and swanse pelssamle chanlorkeathan she  uljars tosy thectang would alre tone their flemes;\n",
      " her uppare for and not moburintste the room to ge; but whung that Gregorr. All in been his ferengoncef \n",
      "----\n",
      "iter 66000, loss: 47.146583\n",
      "----\n",
      " s fcations. It a chat.  CMingont Shacl of frmalion\n",
      "ntectra2ureme it spertiablr) thecf bustiois yould womf andtelt or a would to becach cokiciint\n",
      "\n",
      "Fow.\n",
      " I't extling incopy plate pllainad confont event  \n",
      "----\n",
      "iter 67000, loss: 45.943448\n",
      "----\n",
      " gepine lathers, fest sate sark. Welk offure soot anensers work the shide not theis moppent every timclessagle by plearted he suls ald, would liveliss\", ser. Ir gow\n",
      "\n",
      "Could $fly hild, kear balide if op  \n",
      "----\n",
      "iter 68000, loss: 43.702790\n",
      "----\n",
      " tas'y visowent the moupming thit hopedly stnough them and out and head doored not able ;o wall oraan his sheshing wning for and some dis if it his falr in momiin was tion ho beacled sitiliclest quidua \n",
      "----\n",
      "iter 69000, loss: 42.472928\n",
      "----\n",
      " s thlechel, would mmioras thing lech eftheed, anling towing sore ofly on to sork to a for theshed of they the) for cover pust and, had the conk on tith ace and her if erthe sensice; her could himect h \n",
      "----\n",
      "iter 70000, loss: 42.334129\n",
      "----\n",
      " if the ptore takin but in viod room the the work his she spyeaps of the acorge ammariful on eave now him evein;. Wixsebanite the as and wrom. And sust eveary care withe, up if le thather wardy, the it \n",
      "----\n",
      "iter 71000, loss: 44.402034\n",
      "----\n",
      " aking shoughtare) to kere mole copied comorectieviin mud theck, beroused now asway be their slive for, quld more nevaray bundar abourd.\n",
      "\n",
      "I to thich.1 Fory the rean bade beding ene Gregok ascerent as o \n",
      "----\n",
      "iter 72000, loss: 46.493491\n",
      "----\n",
      " nad it wad bee, rouke haked. OS there stretiind in hadst onfat do haps, as he in, prepate now thesily not not out to all under could of his not from and bed atides dake.  'the tening forstubutt of hat \n",
      "----\n",
      "iter 73000, loss: 44.564450\n",
      "----\n",
      " rces. In him forents letle\"t, beching with and verowly breat hand to rot of back. THor mothougher his hads twad tanded awe? Ther exped to Ane wherhert wwou dly her but who jurdetunge fin sitheseld twi \n",
      "----\n",
      "iter 74000, loss: 42.737142\n",
      "----\n",
      " the fall had dalined moch.\n",
      "\n",
      "Thass itser bectsem. Onk pane wither eringlrtef itilft. ye wteen clerarpel, Gregor's moent and forntory out have flecould had on thrny the ghat evare and greas open yoo glo \n",
      "----\n",
      "iter 75000, loss: 42.460157\n",
      "----\n",
      " ot round somark of melily grarming that sother worem, he spests not thim that would bele up initg thand awer whering fok suncalr for Gregors ucher impted ay of -e mans im the whipptet as fout, but of  \n",
      "----\n",
      "iter 76000, loss: 42.346036\n",
      "----\n",
      " a himle way wen\", seard tho wor's. Shem with heally dild. Gragerst wned dishordly the fromiing be that 3nseargey waveed on aind for agsenting eying him and then these be the starthel to Foor aplion. W \n",
      "----\n",
      "iter 77000, loss: 46.443961\n",
      "----\n",
      " met\n",
      "and the clacatucens purndent wite to clarver atains drations forect eBo chilf, remaces hat, intold\n",
      "dire of reentilicedlamsiag reswarl ching up re, wore got would asmsing nidcineat otiny and nistre \n",
      "----\n",
      "iter 78000, loss: 45.478389\n",
      "----\n",
      "  she fronses in, shone on chay thet ase.\" Ically wosins's viation alre cerid brother ttletly reation\n",
      "Itho in was lound onten the at he rouk had mmiext as sturser of herssed hims, on the. Had op. THem, \n",
      "----\n",
      "iter 79000, loss: 43.263227\n",
      "----\n",
      " ern great withouf the regade bectal has mpating an cond ectien he. Ther gat. At fle the stppove offeired Ist lte coushed in mably in the didcent besepped - had tid frommiclood theme the at it wlows or \n",
      "----\n",
      "iter 80000, loss: 42.078601\n",
      "----\n",
      "  that to reether, casenve not otter Foreft had buch prean ite Gregor a dibet had coud now in hione shoughts a ruint of his look a steak on tlroum; qup the preare gowchalf away onte, speraboably, blout \n",
      "----\n",
      "iter 81000, loss: 41.941963\n",
      "----\n",
      " ockibent theid frojectacory pingter toon, and lookily, formene agrebbinget t reatly of the for at could nevered arlun's before amobly me wedly which, staes commearguinly was laim, right, his thele, th \n",
      "----\n",
      "iter 82000, loss: 43.875357\n",
      "----\n",
      "  to it the to gech spreill was lepy ett. Sirbectle but coud srom thruble bewing in the kile any on they botroke probight ser the Licharing ancemary, appeaces erpurect he hoor as and donded to quegtren \n",
      "----\n",
      "iter 83000, loss: 45.964320\n",
      "----\n",
      " whis swould detmenen gimelenneve that his monderef! PPrommate from  Gregor he whis fless evea erpincaites.\n",
      "\n",
      " werle with, was dist ine be be him and bowing what in laing om ins reectmed it divur.o. Mrt \n",
      "----\n",
      "iter 84000, loss: 44.191602\n",
      "----\n",
      " ook his anss be to to as eppietragull; it away lect , rwagemabiling to the carvessed and, aw tabe to thated whing evell thas nither the saulp had his been: Sadle plerpen wheem.\n",
      " -  trerick eBurbomly,  \n",
      "----\n",
      "iter 85000, loss: 42.356881\n",
      "----\n",
      " hough the comlett ever to ather, exping there. His wite him oleten, wand whaibelf shoutione fouchedteness was saith isle poigg she coslingh to becentw and havibugs Grenob, am glat anthn't all to he to \n",
      "----\n",
      "iter 86000, loss: 42.150976\n",
      "----\n",
      " ll of as any, evely a loor. Wes moth now fernowed to deer bees longed maly on the slow, byen iven ontroonepilpen tacbud agelf edarel ole mas atiting ald  very and the mowhar he laibly wast bey would t \n",
      "----\n",
      "iter 87000, loss: 41.971262\n",
      "----\n",
      " initure naaricure kuor, reghtirnode out was dusen it nit alm in time the surny. He was paite, can appleary, to even hey and go theed taclidet at on aboust was fort awa loved at tim any remise is Grego \n",
      "----\n",
      "iter 88000, loss: 45.887746\n",
      "----\n",
      " ding any that and hurk of aisioustaimemed as a clantice. NTEI/8F F.E.T BFED ANat ppotable, his tht doist 4007 oncerf I mutionsob\n",
      "\n",
      "ThI'm; aistsicentabun or dive Projica enfunss lind for clisacp dade of \n",
      "----\n",
      "iter 89000, loss: 45.064797\n",
      "----\n",
      "  for out anche about on blaclson't a sanlising, thairs. Soork chiltane hush acte seoringly. To chat stome tate 5us he was thincer, of fregher whon it of bus that you, and it clerably oult besy to for  \n",
      "----\n",
      "iter 90000, loss: 42.886944\n",
      "----\n",
      " has held ast, do the light his folit undenet wa head to him. Feet door sow to serick to the foll, dore had seese, held of some oftitem, looke fressiad thatten then not no madely from the vists, was sr \n",
      "----\n",
      "iter 91000, loss: 41.826851\n",
      "----\n",
      " it of a tide loould but havarn the unde, stare tow heand.\n",
      "\n",
      "\n",
      "Thind wittly dist rqock and Gyint Greings,, gooldublectern. \"0E I be sersorled Gregor thar in thess ewhending higge, just to his ons for to  \n",
      "----\n",
      "iter 92000, loss: 41.622436\n",
      "----\n",
      " ad lim, way to go mund the vithed. He not sleatidiss. The more that was nefing to centhe cound he with the dis his sos; (veronplemelsarg. But botieffarging yinst thee beed took's beeg to thadlowels co \n",
      "----\n",
      "iter 93000, loss: 43.460191\n",
      "----\n",
      " ed was think. S I her seemsing they thating had lootis a pute Gregor's so inaventer, shoinhtid becerdy of could and woathove fiddatione shoudy bely to his could the lomaally prever\n",
      "che withidiculd, il \n",
      "----\n",
      "iter 94000, loss: 45.579047\n",
      "----\n",
      " pening\", this llone popyther ge he of anded but apaked youhd has newhly the real ryim not he him any and conting workeddage e\n",
      "cent of of all?\n",
      "He wenliek be tross lef braiks beth ther tist as exstone l \n",
      "----\n",
      "iter 95000, loss: 43.867084\n",
      "----\n",
      " to his posenuct\n",
      "frother craing ione, hildally mut langs, and herdy hes htens.  at with pele by to Cnoded without miturd. The pompad the cherincested about, haveingave sfece. IT  rest the will brat the \n",
      "----\n",
      "iter 96000, loss: 42.054179\n",
      "----\n",
      " could plear to levat himgmoke couplibed the reation woursbor do equind ucund dosen urtiey fell yigle the verow he wark, whion, ap, was no her had did in igh he hew esing our or to eady if she falled p \n",
      "----\n",
      "iter 97000, loss: 41.907548\n",
      "----\n",
      " at abed agaible and ibstures; at a as for on her gow and had seraan somessob, a would not, herole and had un thre they his folille. read unded ont ingore that of the herstabrs flown had opler. alr tra \n",
      "----\n",
      "iter 98000, loss: 41.706290\n",
      "----\n",
      " shandest he seles cuss awm befiire puirit thume and that or teat to frove't fester, mornsand him beplated warr ardecs foly of and, timely him to hen cleshed cof ans there upeabbent. Alm, of they lear  \n",
      "----\n",
      "iter 99000, loss: 45.429210\n",
      "----\n",
      " ed the key and an, scant beinbire to omre into lams veremacl\n",
      "ton's froject\n",
      "Gutmougdery tis at and tut forcentlr somadic. a wank (AI Tis ifw hims\n",
      "proster was and tropaimed wiven lefped maw thempsontanm \n",
      "----\n",
      "iter 100000, loss: 44.697914\n",
      "----\n",
      " te any in liggshed he froping, worke wion thele badt time inga this sisting the.\n",
      "He way. Thele day all's could the ragreft dikesats festish wone out stull notroaingt of tey no yincl stime pred only ca \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "n, p = 0, 0\n",
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad                                                                                                                \n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0                                                                                                                        \n",
    "while n<=1000*100:\n",
    "  # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "  # check \"How to feed the loss function to see how this part works\n",
    "  if p+seq_length+1 >= len(data) or n == 0:\n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory                                                                                                                                      \n",
    "    p = 0 # go from start of data                                                                                                                                                             \n",
    "  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]\n",
    "  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]\n",
    "\n",
    "  # forward seq_length characters through the net and fetch gradient                                                                                                                          \n",
    "  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev)\n",
    "  smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "  # sample from the model now and then                                                                                                                                                        \n",
    "  if n % 1000 == 0:\n",
    "    print('iter %d, loss: %f' % (n, smooth_loss)) # print progress\n",
    "    sample(hprev, inputs[0], 200)\n",
    "\n",
    "  # perform parameter update with Adagrad                                                                                                                                                     \n",
    "  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "    mem += dparam * dparam\n",
    "    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "  p += seq_length # move data pointer                                                                                                                                                         \n",
    "  n += 1 # iteration counter    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
